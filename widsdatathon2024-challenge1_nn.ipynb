{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from utils.eval_helpers import calculate_binary_classification_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "device = (\n",
    "    \"cuda\" \n",
    "    if torch.cuda.is_available() \n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed(device)\n",
    "    torch.cuda.manual_seed_all(device)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "elif device == \"cpu\":\n",
    "    torch.manual_seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def calculate_loss_and_metrics(model, loss_func, x, y, threshold=0.5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_prob = model(x)\n",
    "        loss = loss_func(y_prob, y).item()\n",
    "        y_pred = (y_prob >= threshold).int()\n",
    "        tensors = [y, y_pred, y_prob]\n",
    "        lists = [tensor.cpu().numpy().tolist() for tensor in tensors]\n",
    "        metrics_df = calculate_binary_classification_metrics(*lists)\n",
    "    return loss, metrics_df\n",
    "\n",
    "def fit(epochs, model, train_dl, loss_func, opt, log_dir, x_train, y_train, x_val, y_val):\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            lossb = loss_func(model(xb), yb)\n",
    "            lossb.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        train_loss, train_metrics_df = calculate_loss_and_metrics(model, loss_func, x_train, y_train)\n",
    "        val_loss, val_metrics_df = calculate_loss_and_metrics(model, loss_func, x_val, y_val)\n",
    "\n",
    "        writer.add_scalar('loss/train', train_loss, epoch)\n",
    "        writer.add_scalar('loss/validation', val_loss, epoch)\n",
    "        for column in train_metrics_df.columns:\n",
    "            writer.add_scalar(f'{column}/train', train_metrics_df[column].values, epoch)\n",
    "        for column in val_metrics_df.columns:\n",
    "            writer.add_scalar(f'{column}/validation', val_metrics_df[column].values, epoch)\n",
    "\n",
    "        torch.save(model.state_dict(), Path(log_dir) / Path(f'model_weights_{epoch}.pth'))\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('X_train_minmax_scaled.pkl', 'rb') as file:\n",
    "    X_train_minmax_scaled = pickle.load(file)\n",
    "# with open('X_train_minmax_scaled_resampled.pkl', 'rb') as file:\n",
    "#     X_train_minmax_scaled = pickle.load(file)\n",
    "with open('X_validation_minmax_scaled.pkl', 'rb') as file:\n",
    "    X_validation_minmax_scaled = pickle.load(file)\n",
    "with open('y_train.pkl', 'rb') as file:\n",
    "    y_train = pickle.load(file)\n",
    "# with open('y_train_resampled.pkl', 'rb') as file:\n",
    "#     y_train = pickle.load(file)\n",
    "with open('y_validation.pkl', 'rb') as file:\n",
    "    y_validation = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled = False\n",
    "bs = 128\n",
    "lr = 0.001\n",
    "epochs = 20\n",
    "input_dim = X_train_minmax_scaled.shape[1]\n",
    "output_dim = 1\n",
    "loss_func = nn.BCELoss()\n",
    "log_name = f'resampled{resampled}-bs{bs}-lr{lr}'\n",
    "log_dir = Path('runs') / Path(log_name)\n",
    "\n",
    "if log_dir.exists():\n",
    "    shutil.rmtree(log_dir)\n",
    "    print(f\"Folder '{log_dir}' and all its contents have been deleted.\")\n",
    "else:\n",
    "    print(f\"Folder '{log_dir}' does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t, y_t, x_val, y_val = map(\n",
    "    lambda x: torch.tensor(x, device=device, dtype=torch.float32),\n",
    "    (X_train_minmax_scaled, y_train.reshape(-1, 1), X_validation_minmax_scaled, y_validation.reshape(-1, 1))\n",
    ")\n",
    "train_ds = TensorDataset(x_t, y_t)\n",
    "valid_ds = TensorDataset(x_val, y_val)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True, drop_last=False)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs, shuffle=False, drop_last=False)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_dim, output_dim),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "print(f'#Parameters: {count_parameters(model)}')\n",
    "\n",
    "fit(epochs, model, train_dl, loss_func, optimizer, log_dir, x_t, y_t, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = 'runs/resampledFalse-bs128-lr0.001/model_weights_3.pth'\n",
    "model.load_state_dict(torch.load(weights_path, map_location=torch.device(device), weights_only=True), strict=True)\n",
    "_, metrics_df = calculate_loss_and_metrics(model, loss_func, x_val, y_val)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\n",
    "\n",
    "The best model is logistic-regressor-like, which is trained not using resampling data. Training and validation losses are still high (~0.47). Introducing more parameters (extra linear layers) will degrade the performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Wids2024DevEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
