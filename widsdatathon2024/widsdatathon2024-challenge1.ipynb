{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions for Running This Notebook:\n",
    "- Do not `Run All`.\n",
    "- Instead, follow these steps: \n",
    "    1. Run all cells up to, but not including, the section [Modeling](#modeling)\n",
    "    2. Next, execute the last cell within the section [Data Transformation](#data-transformation)\n",
    "    3. Finally, run the remaining cells in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Overview](https://www.kaggle.com/competitions/widsdatathon2024-challenge1/overview)\n",
    "- Objective: Develop a model to predict if patients received a metastatic cancer diagnosis within 90 days of screening (i.e., `DiagPeriodL90D`) using a unique oncology dataset.\n",
    "- Motivation: \n",
    "    - Metastatic TNBC is considered the most aggressive TNBC and requires most urgent and timely treatment. Unnecessary delays in diagnosis and subsequent treatment can have devastating effects in these difficult cancers. Differences in the wait time to get treatment is a good proxy for disparities in healthcare access.\n",
    "    - The primary goal of building these models is to detect relationships between demographics of the patient with the likelihood of getting timely treatment. The secondary goal is to see if environmental hazards impact proper diagnosis and treatment.\n",
    "- Dataset\n",
    "    - Source: Provided by Gilead Sciences, originating from Health Verity and enriched with third-party geo-demographic data and zip code level toxicology data from NASA/Columbia University.\n",
    "    - Content: Information about demographics, diagnosis, treatment options, and insurance for patients diagnosed with breast cancer from 2015-2018.\n",
    "    - Highlighted Features:\n",
    "        - Demographics (e.g., age, gender, race, ...)\n",
    "        - Diagnosis and treatment details (e.g., breast cancer diagnosis code, metastatic cancer diagnosis code, metastatic cancer treatments, ...)\n",
    "        - Insurance information\n",
    "        - Geo (zip-code level) demographic data (e.g, income, education, rent, race, poverty, ...)\n",
    "        - Toxic air quality (zip-code level) data (e.g., Ozone, PM25 and NO2, ...)\n",
    "- Evaluation\n",
    "    - Metric: Area Under the Receiver Operating Characteristic (ROC) Curve (AUC-ROC).\n",
    "    - Leaderboard:\n",
    "        - During the competition: 51% of the test data.\n",
    "        - Final standings: 49% of the test data.\n",
    "- Submission Format\n",
    "    - File Format: CSV\n",
    "    - Columns:\n",
    "        - `patient_id` (integer)\n",
    "        - `DiagPeriodL90D` (percentage)\n",
    "    - Example:\n",
    "        ```\n",
    "        patient_id,DiagPeriodL90D\n",
    "        372069,.5\n",
    "        981264,.5\n",
    "        ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Dataset](https://www.kaggle.com/competitions/widsdatathon2024-challenge1/data)\n",
    "Roughly 18k records, each corresponds to a single patient and her Diagnosis Period\n",
    "- `training.csv`\n",
    "    - 12906 records\n",
    "    - 83 columns, the last column is `DiagPeriodL90D` (int64)\n",
    "- `test.csv`\n",
    "    - 5792 records\n",
    "    - 82 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pickle\n",
    "import random\n",
    "from typing import List\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from xgboost import XGBClassifier\n",
    "from IPython.display import display\n",
    "\n",
    "from utils.df_helpers import (\n",
    "    find_duplicates, get_value_counts, get_combined_value_counts, highlight_diff, \n",
    "    highlight_nan, plot_category_distribution, plot_side_by_side_histogram_with_percentages\n",
    ")\n",
    "from utils.eval_helpers import (\n",
    "    evaluate_binary_classifier, rank_features_by_importance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "random.seed(random_state)\n",
    "title_fontsize = 14\n",
    "label_fontsize = 12\n",
    "tick_fontsize = 10\n",
    "text_fontsize = 10\n",
    "pd.set_option(\"mode.copy_on_write\", True)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "sns.set_theme(context=\"notebook\", style=\"darkgrid\", palette=\"deep\", font=\"sans-serif\", font_scale=1, color_codes=True, rc=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = pd.read_csv('data/training.csv')\n",
    "df_training.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #57b9ff;\">\n",
    "From the `dtypes` (NumPy data types: `np.float64`, `np.int64`; Python's built-in type: `object`), we know that the missing values in all columns can only be `numpy.nan`, i.e., `NaN`.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #57b9ff;\">\n",
    "There are some records where all attributes are identical except for `patient_id`. After removing these duplicates (keeping the first occurrence), 12,870 records remain.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find duplicates except the `patient_id` column\n",
    "duplicates_df, columns_to_check = find_duplicates(df_training, 'patient_id')\n",
    "first_occurrence_df = duplicates_df.drop_duplicates(subset=columns_to_check)\n",
    "# print('All duplicated rows (including the first occurrence):')\n",
    "# display(duplicates_df)\n",
    "print(f'#Duplicated rows (excluding the first occurrence): {len(duplicates_df) - len(first_occurrence_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "df_training_without_duplicates = df_training.drop_duplicates(subset=columns_to_check, ignore_index=True)\n",
    "print(f'Removed {len(df_training) - len(df_training_without_duplicates)} duplicated rows.')\n",
    "print(f'Current #Patients: {len(df_training_without_duplicates)}')\n",
    "df_training_without_duplicates.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'DiagPeriodL90D'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DiagPeriodL90D`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #57b9ff;\">\n",
    "Approximately 38% of patients did not receive a metastatic cancer diagnosis within 90 days of screening.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_value_counts(df_training_without_duplicates, target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_category_distribution(df_training_without_duplicates, target_column, [(0, '>= 90 Days'), (1, '< 90 Days')], title_fontsize, tick_fontsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `patient_race`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #57b9ff;\">\n",
    "Race information is unknown for approximately 50% of patients. Among the known cases, the only race that comprises more than 10% is White, at about 28%. Patients with unknown races are slightly more likely to be undiagnosed within 90 days of screening, while patients identified as White are slightly more likely to be diagnosed timely.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_nan(get_combined_value_counts(df_training_without_duplicates, 'patient_race', target_column, [0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `payer_type`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #57b9ff;\">\n",
    "Around 47% of patients have commercial insurance. The proportions of patients with Medicaid or Medicare are similar, each close to 20%. Patients with unknown insurance types are slightly more likely to be diagnosed timely, while those with commercial insurance are slightly more likely to be undiagnosed within 90 days of screening.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_nan(get_combined_value_counts(df_training_without_duplicates, 'payer_type', target_column, [0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `patient_state`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #57b9ff;\">\n",
    "The top 6 states where more than 5% of patients come from are CA (18.9%), TX (8.9%), NY (8.1%), MI (6.6%), IL (6.1%), and OH (5.9%). The tail of the distribution includes RI, NH, and MA, each at 0.00777%. Patients from some states, such as CA, MI, MN and CO, tend to receive more timely diagnoses compared to those from other states.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_nan(get_combined_value_counts(df_training_without_duplicates, 'patient_state', target_column, [0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `patient_zip3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #57b9ff;\">\n",
    "There are 739 zip3 codes with patient distribution ranging from 0.00777% to 1.8%. There are no clear differences among zip3 codes in terms of receiving a timely diagnosis.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_zip3_counts_df = get_combined_value_counts(df_training_without_duplicates, 'patient_zip3', target_column, [0, 1])\n",
    "print(f'#Zip3 codes: {len(patient_zip3_counts_df)}')\n",
    "highlight_nan(patient_zip3_counts_df.head(10))\n",
    "# highlight_nan(patient_zip3_counts_df.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `patient_age`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #57b9ff;\">\n",
    "This dataset includes patients ranging from 18 to 91 years old, with more than 80% of the patients aged between 40 and 80. Patients older than 60 are slightly more likely to be diagnosed timely, while younger patients are slightly more likely to be missed.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Min & max age: {df_training_without_duplicates['patient_age'].min()}, {df_training_without_duplicates['patient_age'].max()}')\n",
    "age_boundries = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]  # bins: [10-20), [20-30), [30-40), [40-50), [50-60), [60-70), [70-80), [80-90), [90-100]\n",
    "plot_side_by_side_histogram_with_percentages(df_training_without_duplicates, 'patient_age', target_column, [0, 1], age_boundries, title_fontsize, label_fontsize, text_fontsize, tick_fontsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `patient_gender`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #57b9ff;\">\n",
    "All patients are female.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_nan(get_value_counts(df_training_without_duplicates, 'patient_gender'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `bmi`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #57b9ff;\">\n",
    "Nearly 70% of patients lack BMI information, and these patients are slightly more likely to receive a timely diagnosis compared to those with BMI data. For the available records, BMI ranges from 14.0 to 85.0. Most patients fall into the categories of 'Normal weight', 'Pre-obesity', and 'Obesity class I'. Patients with normal weights are slightly more likely to receive timely diagnoses, whereas those with higher BMIs are less likely to do so.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WHO Classification of BMI\n",
    "\n",
    "| BMI Interval (kg/m²) | Category                |\n",
    "|----------------------|-------------------------|\n",
    "| < 18.5               | Underweight             |\n",
    "| 18.5 - 24.9          | Normal weight           |\n",
    "| 25 - 29.9            | Pre-obesity             |\n",
    "| 30 - 34.9            | Obesity class I         |\n",
    "| 35 - 39.9            | Obesity class II        |\n",
    "| ≥ 40                 | Obesity class III       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmi_df = df_training_without_duplicates['bmi'].isna().to_frame(name='is_bmi_missing')\n",
    "highlight_nan(get_combined_value_counts(pd.merge(bmi_df, df_training_without_duplicates, how='left', left_index=True, right_index=True), 'is_bmi_missing', target_column, [0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmi_boundries = [0, 18.5, 25, 30, 35, 40, 85]  # bins: [0, 18.5), [18.5, 25), [25, 30), [30, 35), [35, 40), [40, 85]\n",
    "id_bmi_df = df_training_without_duplicates[['patient_id', 'bmi']].dropna()\n",
    "id_target_df = df_training_without_duplicates[['patient_id', target_column]]\n",
    "id_bmi_target_df = pd.merge(id_bmi_df, id_target_df, how='left', on='patient_id')\n",
    "plot_side_by_side_histogram_with_percentages(id_bmi_target_df, 'bmi', target_column, [0, 1], bmi_boundries, title_fontsize, label_fontsize, text_fontsize, tick_fontsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `breast_cancer_diagnosis_code`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #57b9ff;\">\n",
    "    <ol>\n",
    "        <li>The number of ICD-10 patients is 3.3 times that of ICD-9 patients. The most common codes are 174.9 (15.3%), C50.911 (13.9%), C50.912 (13.3%), and C50.919 (11.4%). Many other codes have fewer than 10 patients.\n",
    "        <li>Patients with a primary diagnosis code of 174.9 are more than 16 times as likely to remain undiagnosed within 90 days after screening. Generally, patients with ICD-9 codes are less likely to receive a timely diagnosis compared to those with ICD-10 codes.\n",
    "        <li>ICD-10 codes offer more detailed diagnoses for breast cancer compared to ICD-9 codes. While it is possible to map ICD-9 codes to ICD-10 codes, this process can alter the distribution of diagnosis codes. Therefore, direct conversion is not performed. However, some available mappings include: C50.9|174.9, C50.41|174.4, C50.81|174.8, C50.21|174.2, C50.11|174.1, C50.51|174.5, C50.31|174.3, C50.01|174.6.\n",
    "        <li>Some codes in the dataset (C50.929, C50.021, 175.9, C50.421) indicate breast cancer in males, which contradicts the information in the `patient_gender` column. These discrepancies suggest that there may be man-made errors present in the dataset, possibly introduced during data collection or data preparation. These records are not removed but require additional attention.\n",
    "        <li>The code 198.81 represents a secondary malignant neoplasm of the breast, indicating that the cancer has metastasized to the breast from another part of the body. This differs from the other codes in the dataset, which represent primary breast cancer. Only one patient out of nine has `DiagPeriodL90D=1`. This definitely requires attention.\n",
    "    </ol>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_icd10 = df_training_without_duplicates['breast_cancer_diagnosis_code'].str.startswith('C')\n",
    "icd10_df = df_training_without_duplicates[is_icd10]\n",
    "icd9_df = df_training_without_duplicates[~is_icd10]\n",
    "print(f'The ratio of ICD-10 codes to ICD-9 codes: {len(icd10_df)/len(icd9_df):.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_counts_df = get_combined_value_counts(df_training_without_duplicates, 'breast_cancer_diagnosis_code', target_column, [0, 1])\n",
    "code_desc_df = df_training_without_duplicates[['breast_cancer_diagnosis_code', 'breast_cancer_diagnosis_desc']].drop_duplicates()\n",
    "code_counts_desc_df = pd.merge(code_counts_df, code_desc_df, left_index=True, right_on='breast_cancer_diagnosis_code')\n",
    "highlight_nan(code_counts_desc_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edge Cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_codes = ['C50929', 'C50021', '1759', 'C50421']\n",
    "secondary_code = '19881'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_code_df = df_training_without_duplicates[df_training_without_duplicates['breast_cancer_diagnosis_code'].isin(male_codes)]\n",
    "male_code_df[['patient_id', target_column]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondary_code_df = df_training_without_duplicates[df_training_without_duplicates['breast_cancer_diagnosis_code'] == secondary_code]\n",
    "secondary_code_df[['patient_id', target_column]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `breast_cancer_diagnosis_desc`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #57b9ff;\">\n",
    "    <ol>\n",
    "        <li>Many words in the descriptions, such as 'Malignant'/'Malig', 'neoplasm'/'neopl', 'female', and 'breast', are not informative since they appear in almost every field. Removing these words can make the descriptions shorter and more concise. For example, the description 'Malignant neoplasm of breast (female), unspecified' can be reduced to 'unspecified' eventually.\n",
    "        <li>Some words have short forms, and using a uniform representation can reduce unnecessary variations. For this purpose, 'unsp' will be replaced by 'unspecified', and 'ovrlp' will be replaced by 'overlapping'.\n",
    "        <li>Each original description consists of 4 to 10 words. These descriptions specify the position of the neoplasm, such as the quadrant and side. After cleaning, the descriptions have been reduced to 1 to 7 words, with the single one-word description ('of') corresponding to C50.\n",
    "        <li>Through further preprocessing, the text will be converted to lowercase, and common English stop words (e.g., 'of', 'and', etc.) will be removed. Punctuation and special characters will also be eliminated implicitly by the text encoder.\n",
    "    </ol>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tokens(text: str, tokens: List[str]) -> str:\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in tokens]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def replace_tokens(text: str, replacements: dict) -> str:\n",
    "    words = text.split()\n",
    "    replaced_words = [replacements.get(word, word) for word in words]\n",
    "    return ' '.join(replaced_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textual_column = 'breast_cancer_diagnosis_desc'\n",
    "textual_column_cleaned = textual_column + '_cleaned'\n",
    "tokens_to_remove = ['Malignant', 'malignant', 'Malig',\n",
    "                    'neoplasm', 'neoplm', \n",
    "                    'female', '(female),',\n",
    "                    'breast', 'breast,']\n",
    "token_replacements = {\n",
    "    'unsp': 'unspecified',\n",
    "    'ovrlp': 'overlapping'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_counts_desc_df[textual_column_cleaned] = code_counts_desc_df[textual_column].apply(lambda x: remove_tokens(x, tokens_to_remove))\n",
    "code_counts_desc_df[textual_column_cleaned] = code_counts_desc_df[textual_column_cleaned].apply(lambda x: replace_tokens(x, token_replacements))\n",
    "desc_count = code_counts_desc_df[textual_column].apply(lambda text: len(text.split()))\n",
    "desc_cleaned_count = code_counts_desc_df[textual_column_cleaned].apply(lambda text: len(text.split()))\n",
    "print(f'Original #Words: min: {desc_count.min()}, max: {desc_count.max()}')\n",
    "print(f'Current #Words: min: {desc_cleaned_count.min()}, max: {desc_cleaned_count.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_word_code = 'C50'\n",
    "one_word_code_df = df_training_without_duplicates[df_training_without_duplicates['breast_cancer_diagnosis_code'] == one_word_code]\n",
    "one_word_code_df[['patient_id', target_column]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `metastatic_cancer_diagnosis_code`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #57b9ff;\">\n",
    "    <ol>\n",
    "        <li>All of the diagnosis codes are ICD-10 codes. The top 3 codes are C77.3(54.6%; Secondary and unspecified malignant neoplasm of axilla and upper limb lymph nodes), C79.51(14.2%; Secondary malignant neoplasm of bone), C77.9(5.9%; Secondary and unspecified malignant neoplasm of lymph node, unspecified).\n",
    "        <li>Patients with code C77.3 are slightly more likely to be diagnosed timely.\n",
    "        <li>Only 9 out of 43 diagnosis codes have metastatic first novel treatments, and these treatments occur in 24 out of all patients in the dataset (i.e., 0.186%). Among these patients, 8 were not diagnosed within 90 days after screening.\n",
    "    </ol>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_nan(get_combined_value_counts(df_training_without_duplicates, 'metastatic_cancer_diagnosis_code', target_column, [0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metastatic_code_counts_df = get_value_counts(df_training_without_duplicates, 'metastatic_cancer_diagnosis_code')\n",
    "metastatic_code_treatment_df = df_training_without_duplicates[['metastatic_cancer_diagnosis_code', 'metastatic_first_novel_treatment', 'metastatic_first_novel_treatment_type']].drop_duplicates().dropna()\n",
    "metastatic_code_counts_treatment_df = pd.merge(metastatic_code_counts_df, metastatic_code_treatment_df, left_index=True, right_on='metastatic_cancer_diagnosis_code')\n",
    "metastatic_code_counts_treatment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_with_first_novel_treatments_df = df_training_without_duplicates[df_training_without_duplicates['metastatic_first_novel_treatment'].notna()]\n",
    "patients_with_first_novel_treatments_df[['patient_id', target_column]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `metastatic_first_novel_treatment` & `metastatic_first_novel_treatment_type`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #57b9ff;\">\n",
    "There are only two metastatic first novel treatments: PEMBROLIZUMAB and OLAPARIB. Both of these treatments are of the type Antineoplastics.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_nan(get_combined_value_counts(df_training_without_duplicates, 'metastatic_first_novel_treatment', target_column, [0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_nan(get_combined_value_counts(df_training_without_duplicates, 'metastatic_first_novel_treatment_type', target_column, [0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Region`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #57b9ff;\">\n",
    "Patients from the South and Northeast are slightly more likely to remain undiagnosed within 90 days, whereas those from the West and Midwest are slightly more likely to receive a timely diagnosis.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_nan(get_combined_value_counts(df_training_without_duplicates, 'Region', target_column, [0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Division`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #57b9ff;\">\n",
    "Patients from the Pacific, Mountain and West North Central divisions are slightly more likely to receive a timely diagnosis, compared to patients from other divisions.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_nan(get_combined_value_counts(df_training_without_duplicates, 'Division', target_column, [0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `population`, `density`, ..., `veteran`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #57b9ff;\">\n",
    "    <ol>\n",
    "        <li>All patients have complete or partial geo- (zip-code level) demographic data, with the exception of patient ID 224030, whose zip3 code (332) is the only occurrence of this value in the dataset.\n",
    "        <li>In addition to that single patient, three patients with the zip3 code 772 also have missing values in columns starting with 'family', 'income_household', 'home', 'rent', 'self_employed', 'farmer', 'poverty', and 'limited_english'. These three patients are the only ones with this specific zip3 code.\n",
    "    </ol>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_population_df = df_training_without_duplicates[df_training_without_duplicates['population'].isna()]\n",
    "na_population_zip3 = na_population_df['patient_zip3'].unique()\n",
    "print(na_population_zip3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_without_duplicates[df_training_without_duplicates['patient_zip3'] == na_population_zip3[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_family_size_df = df_training_without_duplicates[df_training_without_duplicates['family_size'].isna()]\n",
    "na_family_size_zip3 = na_family_size_df['patient_zip3'].unique()\n",
    "print(na_family_size_zip3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_without_duplicates[df_training_without_duplicates['patient_zip3'] == na_family_size_zip3[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Ozone`, `PM25`, `N02`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #57b9ff;\">\n",
    "29 patients with the zip3 code 967, 968, 998, 995, 996, and 999 lack information about environmental hazards. These patients are the only ones with these specific zip3 codes.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_ozone_df = df_training_without_duplicates[df_training_without_duplicates['Ozone'].isna()]\n",
    "na_ozone_zip3 = na_ozone_df['patient_zip3'].unique()\n",
    "print(na_ozone_zip3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_without_duplicates[df_training_without_duplicates['patient_zip3'].isin(na_ozone_zip3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "| Column & Values                  | **Timely**                            | **Delayed**                                                             |\n",
    "|----------------------------------|---------------------------------------|-------------------------------------------------------------------------|\n",
    "| patient_race                     | White                                 | nan                                                                     |\n",
    "| payer_type                       | nan                                   | COMMERCIAL                                                              |\n",
    "| patient_state                    | CA, MI, MN, CO                        | TX, NY, IL, VA, KY                                                      |\n",
    "| patient_age                      | >60                                   | <=60                                                                    |\n",
    "| bmi                              | nan, <25                              | not nan, >=25                                                           |\n",
    "| breast_cancer_diagnosis_code     | ICD-10                                | 1749, ICD-9                                                             |\n",
    "| metastatic_cancer_diagnosis_code | C773                                  | -                                                                       |\n",
    "| Region                           | West, Midwest                         | South, Northeast                                                        |\n",
    "| Division                         | Pacific, Mountain, West North Central | South Atlantic, Middle Atlantic, West South Central, East South Central |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Edge Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_case_dfs = [male_code_df, secondary_code_df, one_word_code_df, na_family_size_df, na_ozone_df]  # na_population_df is a part of na_family_size_df\n",
    "edge_case_notes = ['male code', 'secondary code', 'one word code', 'na family size', 'na ozone']\n",
    "\n",
    "edge_cases = []\n",
    "for df, note in zip(edge_case_dfs, edge_case_notes):\n",
    "    extracted_df = df[[target_column]]\n",
    "    extracted_df['note'] = note\n",
    "    edge_cases.append(extracted_df)\n",
    "df_edge_cases = pd.concat(edge_cases, ignore_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Linear Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #57b9ff;\">\n",
    "Among all originally numerical columns:\n",
    "    <ol>\n",
    "        <li>Slight positive correlation: e.g., patient_age, education_bachelors, patient_zip3,  income_individual_median, home_value\n",
    "        <li>Slight negative correlation: e.g., education_less_highschool, widowed, income_household_25_to_35, health_uninsured, commute_time\n",
    "        <li>No correlation: race_asian, rent_burden, divorced, N02, race_native, farmer, age_median, home_ownership, veteran, age_60s, never_married\n",
    "    </ol>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = df_training_without_duplicates.corr(numeric_only=True)\n",
    "corr_df[target_column].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new features based on EDA\n",
    "def add_columns(df):\n",
    "    df['is_state_CA_MI_MN_CO'] = df['patient_state'].isin(['CA', 'MI', 'MN', 'CO'])\n",
    "    df['is_state_TX_NY_IL_VA_KY'] = df['patient_state'].isin(['TX', 'NY', 'IL', 'VA', 'KY'])\n",
    "    df['is_age_over_60'] = df['patient_age'] > 60\n",
    "    df['is_bmi_under_25'] = df['bmi'] < 25\n",
    "    df['is_diagnosis_code_ICD10'] = df['breast_cancer_diagnosis_code'].str.startswith('C')\n",
    "    df['is_region_west_midwest'] = df['Region'].isin(['West', 'Midwest'])\n",
    "    df['is_region_south_northeast'] = df['Region'].isin(['South', 'Northeast'])\n",
    "    df['is_division_pacific_mountain_west_north_central'] = df['Division'].isin(['Pacific', 'Mountain', 'West North Central'])\n",
    "    df['is_division_south_atlantic_middle_atlantic_west_south_central_east_south_central'] = df['Division'].isin(['South Atlantic', 'Middle Atlantic', 'West South Central', 'East South Central'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = ['patient_id', 'patient_gender']\n",
    "categorical_columns = ['patient_race', 'payer_type', 'patient_state', 'patient_zip3',  \n",
    "                       'breast_cancer_diagnosis_code', 'Region', 'Division', 'metastatic_cancer_diagnosis_code', \n",
    "                       'metastatic_first_novel_treatment', 'metastatic_first_novel_treatment_type']\n",
    "numerical_columns = df_training_without_duplicates.columns.difference([target_column] + \n",
    "                                                                      columns_to_remove + \n",
    "                                                                      categorical_columns + \n",
    "                                                                      [textual_column]).to_list()\n",
    "unimportant_zip3 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df, is_test_data=False, cat_encoder=None, text_encoder=None):\n",
    "    if is_test_data:\n",
    "        if not cat_encoder or not text_encoder:\n",
    "            raise ValueError(\"Both cat_encoder and text_encoder must be provided when is_test_data is True.\")\n",
    "\n",
    "    # 1. add columns\n",
    "    df_augmented = add_columns(df.copy())\n",
    "    # 1.1 boolean -> int\n",
    "    new_boolean_columns = [col for col in df_augmented.columns if col.startswith('is_')]\n",
    "    for col in new_boolean_columns:\n",
    "        df_augmented[col] = df_augmented[col].astype(int)\n",
    "    \n",
    "    # 2. remove columns\n",
    "    df_reduced = df_augmented.drop(columns=columns_to_remove)\n",
    "\n",
    "\n",
    "    # 3. categorical -> numerical\n",
    "    # 3.1 encode categories\n",
    "    if not is_test_data:\n",
    "        cat_encoder = OneHotEncoder(handle_unknown='infrequent_if_exist', min_frequency=2)\n",
    "        onehot_matrix = cat_encoder.fit_transform(df_reduced[categorical_columns])\n",
    "    else:\n",
    "        onehot_matrix = cat_encoder.transform(df_reduced[categorical_columns])\n",
    "    cat_feature_names = cat_encoder.get_feature_names_out()\n",
    "    onehot_df = pd.DataFrame(onehot_matrix.toarray(), columns=cat_feature_names)\n",
    "    df_no_cat = pd.concat([df_reduced, onehot_df], axis=1).drop(columns=categorical_columns + unimportant_zip3)\n",
    "\n",
    "    # 4. fill NaN in numerical columns with 0.\n",
    "    df_no_cat[numerical_columns] = df_no_cat[numerical_columns].fillna(0.)\n",
    "\n",
    "    # 5. textual -> numerical\n",
    "    # 5.1 clean tokens\n",
    "    df_no_cat[textual_column_cleaned] = df_no_cat[textual_column].apply(lambda x: remove_tokens(x, tokens_to_remove))\n",
    "    df_no_cat[textual_column_cleaned] = df_no_cat[textual_column_cleaned].apply(lambda x: replace_tokens(x, token_replacements))\n",
    "    # 5.2 encode text\n",
    "    if not is_test_data:\n",
    "        text_encoder = TfidfVectorizer(lowercase=True, stop_words='english')\n",
    "        tfidf_matrix = text_encoder.fit_transform(df_no_cat[textual_column_cleaned])\n",
    "    else:\n",
    "        tfidf_matrix = text_encoder.transform(df_no_cat[textual_column_cleaned])\n",
    "    textual_feature_names = text_encoder.get_feature_names_out()\n",
    "    prefix = 'tfidf_'\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=[prefix + name for name in textual_feature_names])\n",
    "    df_prepared = pd.concat([df_no_cat, tfidf_df], axis=1).drop(columns=[textual_column, textual_column_cleaned])    \n",
    "\n",
    "    return df_prepared, cat_encoder, text_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_prepared, cat_encoder, text_encoder = transform(df_training_without_duplicates)\n",
    "df_training_prepared.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-run this and all subsequent cells after the 'Feature Reduction' section has been executed for the first time\n",
    "df_training_prepared.drop(columns=unimportant_zip3, inplace=True)\n",
    "df_training_prepared.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into input and output, train and validation\n",
    "X = df_training_prepared.drop(target_column, axis=1)\n",
    "y = df_training_prepared[target_column]\n",
    "\n",
    "X_train, X_validation, y_train, y_validation, train_indices, validation_indices = train_test_split(\n",
    "    X, y, np.arange(len(X)), test_size=0.2, random_state=random_state\n",
    ")\n",
    "print(f'Training target distribution (after data splitting): {Counter(y_train)}')\n",
    "print(f'Validation target distribution (after data splitting): {Counter(y_validation)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve original data corresponding to validation set\n",
    "df_validation = df_training_without_duplicates.loc[validation_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct balanced validation set\n",
    "X_validation_array = X_validation.to_numpy()\n",
    "y_validation_array = y_validation.to_numpy()\n",
    "all_one_indices = np.where(y_validation_array == 1)[0].tolist()\n",
    "random_one_indices = random.sample(all_one_indices, Counter(y_validation)[0])\n",
    "balanced_indicies = np.where(y_validation_array == 0)[0].tolist() + random_one_indices\n",
    "X_validation_balanced = pd.DataFrame(X_validation_array[balanced_indicies, :], columns=X_validation.columns)\n",
    "y_validation_balanced = pd.Series(y_validation_array[balanced_indicies], name=y_validation.name)\n",
    "print(f'Validation target distribution (after target balancing): {Counter(y_validation_balanced)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_partition(index: int, train_indices: np.ndarray, validation_indices: np.ndarray) -> str:\n",
    "    if index in train_indices:\n",
    "        return 'train'\n",
    "    elif index in validation_indices:\n",
    "        return 'validation'\n",
    "    else:\n",
    "        raise ValueError(f\"Index {index} does not exist in either train_indices or validation_indices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find partitions of edge cases\n",
    "df_edge_cases['partition'] = df_edge_cases.index.map(lambda idx: determine_partition(idx, train_indices, validation_indices))\n",
    "df_edge_cases['probability'] = np.nan\n",
    "df_edge_cases['prediction'] = np.nan\n",
    "edge_case_validation_indices = df_edge_cases[df_edge_cases['partition'] == 'validation'].index.to_list()\n",
    "edge_case_evaluation_indices = [np.where(validation_indices == idx)[0][0] for idx in edge_case_validation_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply standardization (mean=0, variance=1) to the data for logistic regression\n",
    "scaler = StandardScaler()\n",
    "X_train_standard_scaled = scaler.fit_transform(X_train)\n",
    "X_validation_standard_scaled = scaler.transform(X_validation)\n",
    "X_validation_balanced_standard_scaled = scaler.transform(X_validation_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply min-max scaling (min=0, max=1) to the data for neural network\n",
    "scaler = MinMaxScaler()\n",
    "X_train_minmax_scaled = scaler.fit_transform(X_train)\n",
    "X_validation_minmax_scaled = scaler.transform(X_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversample data\n",
    "ros = RandomOverSampler(random_state=random_state)\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "X_train_standard_scaled_resampled, _ = ros.fit_resample(X_train_standard_scaled, y_train)\n",
    "X_train_minmax_scaled_resampled, _ = ros.fit_resample(X_train_minmax_scaled, y_train)\n",
    "print(f'Training target distribution (after data resampling): {Counter(y_train_resampled)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, x_val, y_val, x_val_balanced, y_val_balanced, df_edge_cases, edge_case_evaluation_indices, df_validation, target_column, feature_names=None):\n",
    "    if feature_names is not None:\n",
    "        rank_features_by_importance(feature_names, model.feature_importances_, True)\n",
    "    print('Scores on Imbalanced Validation Set:')\n",
    "    scores_imbalanced_val, df_edge_cases, df_validation = evaluate_binary_classifier(model, x_val, y_val, True, True, df_edge_cases, edge_case_evaluation_indices, True, df_validation)\n",
    "    display(scores_imbalanced_val)\n",
    "    print('Scores on Balanced Validation Set:')\n",
    "    scores_balanced_val, _, _ = evaluate_binary_classifier(model, x_val_balanced, y_val_balanced)\n",
    "    display(scores_balanced_val)\n",
    "    print('Edge Cases:')\n",
    "    display(highlight_diff(df_edge_cases[df_edge_cases['partition'] == 'validation'], target_column, 'prediction'))\n",
    "    print('Samples with High Absolute Errors:')\n",
    "    display(df_validation.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use L1 to get unimportant features\n",
    "model = LogisticRegression(penalty=\"l1\", C=0.01, class_weight='balanced', \n",
    "                           random_state=random_state, solver=\"liblinear\", max_iter=100)\n",
    "model.fit(X_train_standard_scaled, y_train)\n",
    "feature_importance = rank_features_by_importance(feature_names, model.coef_[0])\n",
    "unimportant_features = feature_importance[feature_importance['Importance'] == 0.]['Feature'].to_list()\n",
    "eval(model, X_validation_standard_scaled, y_validation, X_validation_balanced_standard_scaled, y_validation_balanced, df_edge_cases, edge_case_evaluation_indices, df_validation, target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(unimportant_zip3) == 0:\n",
    "    unimportant_zip3 = [f for f in unimportant_features if f.startswith('patient_zip3') and len(f.split('_')[2]) == 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #57b9ff;\">\n",
    "    <ol>\n",
    "        <li>Resampled data was not used due to a degradation in ROC-AUC scores. Instead, setting `class_weight='balanced'` slightly improved the scores for all penalties.\n",
    "        <li>The best-performing model uses L1 as penalty.\n",
    "     </ol>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(penalty=\"l1\", C=0.01, class_weight='balanced', \n",
    "                           random_state=random_state, solver=\"liblinear\", max_iter=100)\n",
    "model.fit(X_train_standard_scaled, y_train)\n",
    "feature_importance = rank_features_by_importance(feature_names, model.coef_[0])\n",
    "print(f'Top 10 Features with Positive Coefficients: \\n{feature_importance.head(10)['Feature'].to_list()}')\n",
    "print(f'Top 10 Features with Negative Coefficients: \\n{feature_importance.tail(10)['Feature'].to_list()}')\n",
    "print(f'All Features with Zero Coefficients: \\n{feature_importance[feature_importance['Importance'] == 0.]['Feature'].to_list()}\\n')\n",
    "eval(model, X_validation_standard_scaled, y_validation, X_validation_balanced_standard_scaled, y_validation_balanced, df_edge_cases, edge_case_evaluation_indices, df_validation, target_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(penalty=\"l2\", C=0.001, class_weight='balanced', \n",
    "                           random_state=random_state, solver=\"liblinear\", max_iter=100)\n",
    "model.fit(X_train_standard_scaled, y_train)\n",
    "feature_importance = rank_features_by_importance(feature_names, model.coef_[0])\n",
    "print(f'Top 10 Features with Positive Coefficients: \\n{feature_importance.head(10)['Feature'].to_list()}')\n",
    "print(f'Top 10 Features with Negative Coefficients: \\n{feature_importance.tail(10)['Feature'].to_list()}')\n",
    "eval(model, X_validation_standard_scaled, y_validation, X_validation_balanced_standard_scaled, y_validation_balanced, df_edge_cases, edge_case_evaluation_indices, df_validation, target_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elasticnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(penalty=\"elasticnet\", C=0.005, class_weight='balanced', \n",
    "                           random_state=random_state, solver=\"saga\", max_iter=500, l1_ratio=0.5)\n",
    "model.fit(X_train_standard_scaled, y_train)\n",
    "feature_importance = rank_features_by_importance(feature_names, model.coef_[0])\n",
    "print(f'Top 10 Features with Positive Coefficients: \\n{feature_importance.head(10)['Feature'].to_list()}')\n",
    "print(f'Top 10 Features with Negative Coefficients: \\n{feature_importance.tail(10)['Feature'].to_list()}')\n",
    "eval(model, X_validation_standard_scaled, y_validation, X_validation_balanced_standard_scaled, y_validation_balanced, df_edge_cases, edge_case_evaluation_indices, df_validation, target_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #57b9ff;\">\n",
    "Again, not using resampled data results in improved performance.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=random_state)\n",
    "model.fit(X_train, y_train)\n",
    "print(f'Depth of the tree: {model.get_depth()}')\n",
    "print(f'Number of leaves: {model.get_n_leaves()}')\n",
    "eval(model, X_validation, y_validation, X_validation_balanced, y_validation_balanced, df_edge_cases, edge_case_evaluation_indices, df_validation, target_column, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(100, 50))\n",
    "plot_tree(model, feature_names=feature_names, filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #57b9ff;\">\n",
    "The model performs best when `class_weight=None` and all samples are used to train each base estimator without resampling the data.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=250, criterion='gini', max_depth=5, \n",
    "                               oob_score=True, random_state=random_state, \n",
    "                               class_weight=None, max_samples=None)\n",
    "model.fit(X_train, y_train)\n",
    "print(f'Out-of-bag score: {model.oob_score_}')\n",
    "eval(model, X_validation, y_validation, X_validation_balanced, y_validation_balanced, df_edge_cases, edge_case_evaluation_indices, df_validation, target_column, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #57b9ff;\">\n",
    "    <ol>\n",
    "        <li>Again, not using resampled data works better.\n",
    "        <li>It performs better than the other models.\n",
    "     </ol>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(n_estimators=90, max_depth=4, learning_rate=0.005, \n",
    "                      objective=\"binary:logistic\", booster=\"gbtree\", \n",
    "                      reg_alpha=4.0, reg_lambda=1.5, random_state=random_state)\n",
    "model.fit(X_train, y_train)\n",
    "eval(model, X_validation, y_validation, X_validation_balanced, y_validation_balanced, df_edge_cases, edge_case_evaluation_indices, df_validation, target_column, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('X_train_minmax_scaled.pkl', 'wb') as file:\n",
    "    pickle.dump(X_train_minmax_scaled, file)\n",
    "with open('X_train_minmax_scaled_resampled.pkl', 'wb') as file:\n",
    "    pickle.dump(X_train_minmax_scaled_resampled, file)\n",
    "with open('X_validation_minmax_scaled.pkl', 'wb') as file:\n",
    "    pickle.dump(X_validation_minmax_scaled, file)\n",
    "with open('y_train.pkl', 'wb') as file:\n",
    "    pickle.dump(y_train.values, file)\n",
    "with open('y_train_resampled.pkl', 'wb') as file:\n",
    "    pickle.dump(y_train_resampled.values, file)\n",
    "with open('y_validation.pkl', 'wb') as file:\n",
    "    pickle.dump(y_validation.values, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code and results are in the notebook [`widsdatathon2024-challenge1_nn.ipynb`](widsdatathon2024-challenge1_nn.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrain on all training data\n",
    "model = XGBClassifier(n_estimators=90, max_depth=4, learning_rate=0.005, \n",
    "                      objective=\"binary:logistic\", booster=\"gbtree\", \n",
    "                      reg_alpha=4.0, reg_lambda=1.5, random_state=random_state)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare test data\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "df_test_prepared, _, _ = transform(df_test, True, cat_encoder, text_encoder)\n",
    "df_test_prepared.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get result\n",
    "result = model.predict_proba(df_test_prepared)[:, 1]\n",
    "df_result = pd.DataFrame({\n",
    "    'patient_id': df_test['patient_id'],\n",
    "    'DiagPeriodL90D': result\n",
    "})\n",
    "df_result.to_csv('test_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Key principles applied in this project include:\n",
    "- Retain records with missing values (as the pattern of such data absences might reflect specific characteristics of patients that could be relevant to the target prediction)\n",
    "- Tune hyperparameters only using a self-constructed validation set (reserving the test data for evaluation only after the best model has been determined)\n",
    "\n",
    "Results:\n",
    "\n",
    "| Positive                                        | Negative                                    |\n",
    "|-------------------------------------------------|---------------------------------------------|\n",
    "| `is_diagnosis_code_ICD10`                       | -                                           |\n",
    "| `patient_age`                                   | -                                           |\n",
    "| -                                               | `metastatic_cancer_diagnosis_code_C7981`    |\n",
    "| `is_state_CA_MI_MN_CO`                          | -                                           |\n",
    "| `payer_type_nan`                                | -                                           |\n",
    "| -                                               | `health_uninsured`                          |\n",
    "| `breast_cancer_diagnosis_code_[C50811, C50011]` | `breast_cancer_diagnosis_code_[1744, 1749]` |\n",
    "| `is_region_west_midwest`                        | -                                           |\n",
    "| -                                               | `patient_state_VA`                          |\n",
    "| `patient_zip3_902`                              | `patient_zip3_[948, 130, 928, 957]`         |\n",
    "| -                                               | `patient_race_Black`                        |\n",
    "| `tfidf_[right, left, overlapping, site]`        | `tfdif_unspecified`                         |\n",
    "\n",
    "- Based on the important features identified across each model, the following are nearly universal: `is_diagnosis_code_ICD10`, `patient_age`, and `metastatic_cancer_diagnosis_code_C7981`. The table above shows features that hold high importance in at least two models, categorized by their impact on timely diagnosis—whether positive (larger values indicate more timely diagnosis) or negative (larger values indicate less timely diagnosis). \n",
    "\n",
    "- The results align closely with the findings from the exploratory data analysis (EDA). Features such as `is_diagnosis_code_ICD10`, `is_state_CA_MI_MN_CO`, `payer_type_nan`, and `is_region_west_midwest` confirm expectations. Additionally, the models have identified new significant features, including `health_uninsured`, `patient_state_VA`, and `patient_race_Black`. The models also highlight more granular elements, such as specific zip3 codes, diagnosis codes, and description tokens, which are consistent with EDA insights. For instance, tokens like `right`, `left`, `overlapping`, and `site` are uniquely associated with ICD-10 codes.\n",
    "\n",
    "- The highest ROC-AUC score is achieved by the XGBoost model, with scores of 0.801685 on the imbalanced validation set and 0.802429 on the balanced validation set. However, the recall for class 0 (DiagPeriodL90D=0) is low, indicating that the model tends to predict more patients as being diagnosed timely, likely due to the imbalanced training data. Resampling did not improve this aspect.\n",
    "\n",
    "- Several geodemographic factors influencing diagnosis wait times highlight disparities in healthcare access. These factors include specific regions (West, Midwest), states (CA, MI, MN, CO), and zip3 codes (902, 948, 130, 928, 957). Regarding environmental hazards, while Ozone, PM25, and NO2 have marginal impacts on diagnosis timing, Ozone shows a slightly greater influence compared to the others.\n",
    "\n",
    "Areas for further exploration:\n",
    "- Alternative text encoders\n",
    "- Different imputation methods for handling NaNs in numerical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
